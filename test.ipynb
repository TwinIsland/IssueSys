{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-16T16:02:09.519434200Z",
     "start_time": "2023-07-16T16:02:07.474137900Z"
    }
   },
   "outputs": [],
   "source": [
    "# encoding=gbk\n",
    "import configparser\n",
    "import time\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Callable\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pickle\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "CONFIG_PATH = \"./config.ini\"\n",
    "\n",
    "\n",
    "class VecSim:\n",
    "    # cos similarity\n",
    "    @staticmethod\n",
    "    def cos_sim(a: Tensor, b: Tensor):\n",
    "        \"\"\"\n",
    "        Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n",
    "        :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n",
    "        \"\"\"\n",
    "        if not isinstance(a, torch.Tensor):\n",
    "            a = torch.tensor(a)\n",
    "\n",
    "        if not isinstance(b, torch.Tensor):\n",
    "            b = torch.tensor(b)\n",
    "\n",
    "        if len(a.shape) == 1:\n",
    "            a = a.unsqueeze(0)\n",
    "\n",
    "        if len(b.shape) == 1:\n",
    "            b = b.unsqueeze(0)\n",
    "\n",
    "        a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n",
    "        b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n",
    "        return float(torch.mm(a_norm, b_norm.transpose(0, 1)))\n",
    "\n",
    "\n",
    "def process_bar(num, total):\n",
    "    rate = float(num) / total\n",
    "    rate_num = int(100 * rate)\n",
    "    r = '\\r[{}{}]{}%'.format('*' * rate_num, ' ' * (100 - rate_num), rate_num)\n",
    "    sys.stdout.write(r)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, question_data: list, answer_data: list, cache_addr: str):\n",
    "        self.__question_data = question_data\n",
    "        self.__answer_data = answer_data\n",
    "        self.__cp = configparser.ConfigParser()\n",
    "        self.__cp.read(CONFIG_PATH)\n",
    "        self.embedding: list[tuple] = []\n",
    "        self.__cache_addr = cache_addr\n",
    "        # Load model from HuggingFace Hub\n",
    "        configs = configparser.ConfigParser()\n",
    "        configs.read(\"config.ini\")\n",
    "        # Load model\n",
    "        self.__tokenizer = AutoTokenizer.from_pretrained(configs['model']['embedding_path'])\n",
    "        self.__model = AutoModel.from_pretrained(configs['model']['embedding_path'])\n",
    "\n",
    "    @staticmethod\n",
    "    def __mean_pooling(model_output, attention_mask):\n",
    "        token_embeddings = model_output[0]  # First element of model_output contains all token embeddings\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "    def get_embedding(self, chunk: str):\n",
    "        # Tokenize sentences\n",
    "        encoded_input = self.__tokenizer(chunk, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "        # Compute token embeddings\n",
    "        with torch.no_grad():\n",
    "            model_output = self.__model(**encoded_input)\n",
    "        # Perform pooling. In this case, max pooling.\n",
    "        return self.__mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "    def processing(self):\n",
    "        if os.path.isfile(self.__cache_addr):\n",
    "            cache = pickle.load(open(self.__cache_addr, 'rb'))\n",
    "            self.embedding = cache\n",
    "        else:\n",
    "            total_chunk = len(self.__question_data)\n",
    "            for chunk_idx, chunk in enumerate(self.__question_data):\n",
    "                self.embedding.append((chunk, self.get_embedding(chunk), answers[chunk_idx]))\n",
    "                process_bar(chunk_idx, total_chunk - 1)\n",
    "            pickle.dump(self.embedding, open(self.__cache_addr, \"wb\"))\n",
    "\n",
    "    def fetch_similarity(self, query: str, similarity_func: Callable = VecSim.cos_sim):\n",
    "        query_vec = self.get_embedding(query)\n",
    "        for em in self.embedding:\n",
    "            yield similarity_func(query_vec, em[1])\n",
    "\n",
    "    def get_embedding_data(self, idx: int):\n",
    "        return self.embedding[idx][0], self.embedding[idx][2]\n",
    "\n",
    "    def rm_cache(self):\n",
    "        if os.path.isfile(self.__cache_addr):\n",
    "            os.remove(self.__cache_addr)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Start here\n",
    "load your data in the next column to avoid running out of memory"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding in progress...\n",
      "[****************************************************************************************************]100%\n",
      "loading llm...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99cf898004764a4ea077071f767e3b05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish!\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(CONFIG_PATH)\n",
    "\n",
    "with open(\"data.bin\", 'rb') as f:\n",
    "    questions, answers, _ = pickle.load(f)\n",
    "\n",
    "# embedding knowledge dataset\n",
    "embedder = Embedder(question_data=questions, answer_data=answers, cache_addr=\"cache\")\n",
    "embedder.rm_cache()  # remove cache if you need, comment it out once embedding data for the first time\n",
    "print(\"embedding in progress...\")\n",
    "embedder.processing()\n",
    "\n",
    "# loading llm model\n",
    "print(\"\\nloading llm...\")\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(config['model']['llm_path'], trust_remote_code=True)\n",
    "llm_model = AutoModel.from_pretrained(config['model']['llm_path'], trust_remote_code=True).cuda()\n",
    "llm_model = llm_model.eval()\n",
    "print('finish!')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T16:02:46.756319300Z",
     "start_time": "2023-07-16T16:02:10.581960300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "similarity_vec = []\n",
    "query = input(\"your query: \")\n",
    "for i in embedder.fetch_similarity(query, VecSim.cos_sim):\n",
    "    similarity_vec.append(i)\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read(CONFIG_PATH)\n",
    "\n",
    "select_item_num = min(len(similarity_vec), int(config['model']['max_item']))\n",
    "top_index = np.argsort(similarity_vec)[-select_item_num:][::-1]\n",
    "\n",
    "template = open(\"template\", \"rb\").read().decode(\"utf-8\").split('%')\n",
    "\n",
    "llm_input = template[0].replace(\"{1}\", query)\n",
    "\n",
    "matched_text = \"\"\n",
    "possible_answer = {}\n",
    "for ele_idx, idx in enumerate(top_index):\n",
    "    if similarity_vec[idx] < float(config['model']['min_sup']):\n",
    "        break\n",
    "    cur_matched = embedder.get_embedding_data(idx)\n",
    "    possible_answer[ele_idx+1] = cur_matched\n",
    "    matched_text += f\"{ele_idx+1}: {cur_matched[0]}答案是{cur_matched[1]} \"\n",
    "\n",
    "llm_input = llm_input.replace(\"{2}\", matched_text)\n",
    "template[0] = llm_input"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T16:04:12.960771400Z",
     "start_time": "2023-07-16T16:04:08.138763200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: xxx是谁\n",
      "BOT: thinking...\n",
      "BOT: 爱化妆的女人,新品种,孔子与孟子有何不同?孔子的子在左边,孟子的子在右边,中国古贤人曾将兰色外衣,浸泡于黄河中,结果产生沾湿。\n",
      "\n",
      "Find similar question:\n",
      "1: 谁是世界上最有恒心的画家？ \n",
      "2: 黃皮肤的人是黃种人，绿皮肤的人属于哪一种？ \n",
      "3: 孔子与孟子有何不同？ \n",
      "4: 孔子与孟子有何不同？ \n",
      "5: 中国古贤人曾将兰色外衣，浸泡于黄河中，结果产生何种现象？ \n",
      "USER: 1\n",
      "BOT: 爱化妆的女人\n",
      "USER: 2\n",
      "BOT: 新品种\n",
      "USER: 3\n",
      "BOT: “子”的位置不同。孔子的“子”在左边，孟子的“子”\n"
     ]
    }
   ],
   "source": [
    "print(f\"USER: {query}\")\n",
    "history = []\n",
    "current_length = 0\n",
    "\n",
    "total_update = len(template)\n",
    "print(\"BOT: thinking...\")\n",
    "for i in range(total_update):\n",
    "    if i == total_update-1:\n",
    "        print(\"BOT: \", end='')\n",
    "        for response, _ in llm_model.stream_chat(llm_tokenizer, template[i],history=history, temperature=0.001):\n",
    "            print(response[current_length:], end='', flush=True)\n",
    "            current_length = len(response)\n",
    "    else:\n",
    "        for response, history in llm_model.stream_chat(llm_tokenizer, template[i],temperature=0.001):\n",
    "            current_length = len(response)\n",
    "    current_length = 0\n",
    "\n",
    "print(\"\\n\\nFind similar question:\", flush=True)\n",
    "for k, v in possible_answer.items():\n",
    "    print(f\"{k}: {v[0]}\", flush=True)\n",
    "while True:\n",
    "    time.sleep(0.3)\n",
    "    dec = input(\"USER: \")\n",
    "    try:\n",
    "        if dec == \"exit\":\n",
    "            break\n",
    "        if int(dec) <= 0 or int(dec) >= len(possible_answer) + 1:\n",
    "            raise Exception\n",
    "        print(f\"USER: {dec}\")\n",
    "        print(f\"BOT: {possible_answer[int(dec)][1]}\")\n",
    "    except Exception:\n",
    "        print(\"BOT: 输入不合法！\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-16T16:05:36.793783400Z",
     "start_time": "2023-07-16T16:04:13.718110600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-07-16T16:01:39.960163400Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
